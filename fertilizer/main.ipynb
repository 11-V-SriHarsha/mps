{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    # Create interaction features between nutrients\n",
    "    df['N_P_ratio'] = df['Nitrogen'] / (df['Phosphorus'] + 1)  # Adding 1 to avoid division by zero\n",
    "    df['N_K_ratio'] = df['Nitrogen'] / (df['Potassium'] + 1)\n",
    "    df['P_K_ratio'] = df['Phosphorus'] / (df['Potassium'] + 1)\n",
    "    df['NPK_sum'] = df['Nitrogen'] + df['Phosphorus'] + df['Potassium']\n",
    "    \n",
    "    # pH interactions (optimal nutrient availability occurs at specific pH ranges)\n",
    "    df['pH_squared'] = df['pH'] ** 2\n",
    "    df['N_pH_interaction'] = df['Nitrogen'] * df['pH']\n",
    "    df['P_pH_interaction'] = df['Phosphorus'] * df['pH']\n",
    "    df['K_pH_interaction'] = df['Potassium'] * df['pH']\n",
    "    \n",
    "    # Temperature and Rainfall features\n",
    "    df['Temp_Rain_ratio'] = df['Temperature'] / (df['Rainfall'] + 1)\n",
    "    \n",
    "    # Handle outliers in numerical columns\n",
    "    num_cols = ['Nitrogen', 'Phosphorus', 'Potassium', 'pH', 'Rainfall', 'Temperature']\n",
    "    for col in num_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = engineer_features(df)\n",
    "\n",
    "label_encoders = {}\n",
    "categorical_columns = ['District_Name', 'Soil_color', 'Crop', 'Fertilizer']\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target\n",
    "# Include engineered features\n",
    "feature_columns = [\n",
    "    'District_Name', 'Soil_color', 'Nitrogen', 'Phosphorus', 'Potassium', \n",
    "    'pH', 'Rainfall', 'Temperature', 'Crop', 'N_P_ratio', 'N_K_ratio', \n",
    "    'P_K_ratio', 'NPK_sum', 'pH_squared', 'N_pH_interaction', \n",
    "    'P_pH_interaction', 'K_pH_interaction', 'Temp_Rain_ratio'\n",
    "]\n",
    "X = df[feature_columns]\n",
    "y = df['Fertilizer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()  # Better handles outliers than StandardScaler\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features for soil properties\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 86 out of 171 features\n"
     ]
    }
   ],
   "source": [
    "# Advanced feature selection using RandomForestClassifier with tuned parameters\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42)\n",
    "rf.fit(X_poly, y)\n",
    "importances = rf.feature_importances_\n",
    "selector = SelectFromModel(rf, threshold=\"median\", prefit=True)\n",
    "X_selected = selector.transform(X_poly)\n",
    "selected_features = selector.get_support(indices=True)\n",
    "print(f\"Selected {len(selected_features)} out of {X_poly.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    \n",
    "    # Define model with trial hyperparameters\n",
    "    model = tf.keras.Sequential([\n",
    "        Bidirectional(LSTM(lstm_units_1, return_sequences=True, input_shape=(1, X_selected.shape[1]))),\n",
    "        Dropout(dropout_rate),\n",
    "        BatchNormalization(),\n",
    "        Bidirectional(LSTM(lstm_units_2)),\n",
    "        Dropout(dropout_rate),\n",
    "        BatchNormalization(),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate/2),\n",
    "        Dense(len(np.unique(y)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Setup cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv.split(X_selected, y):\n",
    "        X_train_cv, X_val_cv = X_selected[train_idx], X_selected[val_idx]\n",
    "        y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        X_train_cv_flat = X_train_cv.reshape(X_train_cv.shape[0], -1)\n",
    "        \n",
    "        # Safer SMOTE application with fallback\n",
    "        try:\n",
    "            # Check if SMOTE is applicable\n",
    "            unique_classes = np.unique(y_train_cv)\n",
    "            class_counts = [np.sum(y_train_cv == cls) for cls in unique_classes]\n",
    "            \n",
    "            # Only apply SMOTE if there are enough samples\n",
    "            if all(count > 5 for count in class_counts):\n",
    "                smote = SMOTE(random_state=42, k_neighbors=min(5, min(class_counts)-1))\n",
    "                X_train_cv_flat, y_train_cv = smote.fit_resample(X_train_cv_flat, y_train_cv)\n",
    "            \n",
    "            # Reshape for LSTM\n",
    "            X_train_cv = X_train_cv_flat.reshape(X_train_cv_flat.shape[0], 1, X_selected.shape[1])\n",
    "            X_val_cv = X_val_cv.reshape(X_val_cv.shape[0], 1, X_selected.shape[1])\n",
    "            \n",
    "            # Get class weights\n",
    "            class_weights = compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(y_train_cv),\n",
    "                y=y_train_cv\n",
    "            )\n",
    "            class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_train_cv, y_train_cv,\n",
    "                epochs=50,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(X_val_cv, y_val_cv),\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                class_weight=class_weight_dict,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            _, accuracy = model.evaluate(X_val_cv, y_val_cv, verbose=0)\n",
    "            cv_scores.append(accuracy)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in trial: {e}\")\n",
    "            return 0.5  # Return a default score if SMOTE fails\n",
    "    \n",
    "    # Return mean accuracy across folds\n",
    "    return np.mean(cv_scores) if cv_scores else 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_oversample(X, y, min_samples=50):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Custom oversampling that ensures minimum samples per class\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    unique_classes = np.unique(y)\n",
    "\n",
    "    X_resampled = X.copy()\n",
    "\n",
    "    y_resampled = y.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    for cls in unique_classes:\n",
    "\n",
    "        cls_mask = y == cls\n",
    "\n",
    "        cls_X = X[cls_mask]\n",
    "\n",
    "        cls_y = y[cls_mask]\n",
    "\n",
    "        \n",
    "\n",
    "        # If class has fewer samples than min_samples, oversample\n",
    "\n",
    "        if len(cls_X) < min_samples:\n",
    "\n",
    "            # Compute how many samples we need to add\n",
    "\n",
    "            n_to_add = min_samples - len(cls_X)\n",
    "\n",
    "            \n",
    "\n",
    "            # If not enough samples for standard oversampling, use simple repetition\n",
    "\n",
    "            if len(cls_X) <= n_to_add:\n",
    "\n",
    "                repeat_indices = np.random.choice(len(cls_X), n_to_add, replace=True)\n",
    "\n",
    "                added_X = cls_X[repeat_indices]\n",
    "\n",
    "                added_y = cls_y[repeat_indices]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Use RandomOverSampler\n",
    "\n",
    "                ros = RandomOverSampler(sampling_strategy={cls: min_samples})\n",
    "\n",
    "                added_X, added_y = ros.fit_resample(cls_X, cls_y)\n",
    "\n",
    "                added_X = added_X[len(cls_X):]\n",
    "\n",
    "                added_y = added_y[len(cls_y):]\n",
    "\n",
    "            \n",
    "\n",
    "            # Concatenate original and new samples\n",
    "\n",
    "            X_resampled = np.vstack([X_resampled, added_X])\n",
    "\n",
    "            y_resampled = np.concatenate([y_resampled, added_y])\n",
    "\n",
    "    \n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameter suggestions\n",
    "\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128)\n",
    "\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64)\n",
    "\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64)\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    \n",
    "\n",
    "    # Model architecture\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "\n",
    "        tf.keras.layers.Bidirectional(\n",
    "\n",
    "            tf.keras.layers.LSTM(lstm_units_1, return_sequences=True), \n",
    "\n",
    "            input_shape=(1, X_selected.shape[1])\n",
    "\n",
    "        ),\n",
    "\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units_2)),\n",
    "\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dropout(dropout_rate/2),\n",
    "\n",
    "        tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "\n",
    "    ])\n",
    "\n",
    "    \n",
    "\n",
    "    # Compile\n",
    "\n",
    "    model.compile(\n",
    "\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "\n",
    "        metrics=['accuracy']\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Cross-validation\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    \n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_selected, y):\n",
    "\n",
    "        X_train_cv = X_selected[train_idx]\n",
    "\n",
    "        X_val_cv = X_selected[val_idx]\n",
    "\n",
    "        y_train_cv = y[train_idx]\n",
    "\n",
    "        y_val_cv = y[val_idx]\n",
    "\n",
    "        \n",
    "\n",
    "        # Flatten for preprocessing\n",
    "\n",
    "        X_train_cv_flat = X_train_cv.reshape(X_train_cv.shape[0], -1)\n",
    "\n",
    "        \n",
    "\n",
    "        try:\n",
    "\n",
    "            # Custom oversampling\n",
    "\n",
    "            X_train_cv_resampled, y_train_cv_resampled = custom_oversample(X_train_cv_flat, y_train_cv)\n",
    "\n",
    "            \n",
    "\n",
    "            # Reshape for LSTM\n",
    "\n",
    "            X_train_cv_resampled = X_train_cv_resampled.reshape(\n",
    "\n",
    "                X_train_cv_resampled.shape[0], 1, X_selected.shape[1]\n",
    "\n",
    "            )\n",
    "\n",
    "            X_val_cv = X_val_cv.reshape(X_val_cv.shape[0], 1, X_selected.shape[1])\n",
    "\n",
    "            \n",
    "\n",
    "            # Compute class weights\n",
    "\n",
    "            class_weights = compute_class_weight(\n",
    "\n",
    "                class_weight='balanced',\n",
    "\n",
    "                classes=np.unique(y_train_cv_resampled),\n",
    "\n",
    "                y=y_train_cv_resampled\n",
    "\n",
    "            )\n",
    "\n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "            \n",
    "\n",
    "            # Callbacks\n",
    "\n",
    "            early_stopping = EarlyStopping(\n",
    "\n",
    "                monitor='val_loss', \n",
    "\n",
    "                patience=10, \n",
    "\n",
    "                restore_best_weights=True\n",
    "\n",
    "            )\n",
    "\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "\n",
    "                monitor='val_loss', \n",
    "\n",
    "                factor=0.5, \n",
    "\n",
    "                patience=5, \n",
    "\n",
    "                min_lr=1e-6\n",
    "\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            # Train\n",
    "\n",
    "            history = model.fit(\n",
    "\n",
    "                X_train_cv_resampled, \n",
    "\n",
    "                y_train_cv_resampled,\n",
    "\n",
    "                epochs=50,\n",
    "\n",
    "                batch_size=batch_size,\n",
    "\n",
    "                validation_data=(X_val_cv, y_val_cv),\n",
    "\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "\n",
    "                class_weight=class_weight_dict,\n",
    "\n",
    "                verbose=0\n",
    "\n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            # Evaluate\n",
    "\n",
    "            _, accuracy = model.evaluate(X_val_cv, y_val_cv, verbose=0)\n",
    "\n",
    "            cv_scores.append(accuracy)\n",
    "\n",
    "        \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in trial: {e}\")\n",
    "\n",
    "            return 0.5\n",
    "\n",
    "    \n",
    "\n",
    "    return np.mean(cv_scores) if cv_scores else 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-22 15:12:54,552] A new study created in memory with name: no-name-2afe67ef-98af-4b2a-be61-471cf09ea131\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:14:08,928] Trial 0 finished with value: 0.5903077483177185 and parameters: {'lstm_units_1': 97, 'lstm_units_2': 53, 'dense_units': 54, 'dropout_rate': 0.2997598033347555, 'learning_rate': 0.0010438136518376958, 'batch_size': 64}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:15:44,523] Trial 1 finished with value: 0.45381519198417664 and parameters: {'lstm_units_1': 79, 'lstm_units_2': 41, 'dense_units': 30, 'dropout_rate': 0.26370083166733105, 'learning_rate': 0.00010026406354503665, 'batch_size': 64}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:18:00,742] Trial 2 finished with value: 0.44982038140296937 and parameters: {'lstm_units_1': 44, 'lstm_units_2': 57, 'dense_units': 59, 'dropout_rate': 0.47147192664646453, 'learning_rate': 0.000747808098711804, 'batch_size': 32}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:19:18,034] Trial 3 finished with value: 0.573025131225586 and parameters: {'lstm_units_1': 128, 'lstm_units_2': 22, 'dense_units': 41, 'dropout_rate': 0.2646734878277045, 'learning_rate': 0.001181224884891744, 'batch_size': 64}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:21:48,785] Trial 4 finished with value: 0.4815019190311432 and parameters: {'lstm_units_1': 42, 'lstm_units_2': 61, 'dense_units': 25, 'dropout_rate': 0.38912026885573503, 'learning_rate': 0.0017063463364910675, 'batch_size': 16}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:25:22,862] Trial 5 finished with value: 0.5032257616519928 and parameters: {'lstm_units_1': 112, 'lstm_units_2': 43, 'dense_units': 22, 'dropout_rate': 0.26937487036360097, 'learning_rate': 0.00015189547809569865, 'batch_size': 16}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:25:55,110] Trial 6 finished with value: 0.36915897727012636 and parameters: {'lstm_units_1': 41, 'lstm_units_2': 23, 'dense_units': 33, 'dropout_rate': 0.46213182716732126, 'learning_rate': 0.004877629371646723, 'batch_size': 64}. Best is trial 0 with value: 0.5903077483177185.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:29:18,123] Trial 7 finished with value: 0.5993961930274964 and parameters: {'lstm_units_1': 98, 'lstm_units_2': 61, 'dense_units': 29, 'dropout_rate': 0.23083163128700046, 'learning_rate': 0.005256760984512994, 'batch_size': 16}. Best is trial 7 with value: 0.5993961930274964.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:30:20,060] Trial 8 finished with value: 0.7857781291007996 and parameters: {'lstm_units_1': 121, 'lstm_units_2': 55, 'dense_units': 39, 'dropout_rate': 0.12621682923609906, 'learning_rate': 0.0002043779024286192, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:31:35,505] Trial 9 finished with value: 0.4176880240440369 and parameters: {'lstm_units_1': 91, 'lstm_units_2': 26, 'dense_units': 27, 'dropout_rate': 0.4188031739091933, 'learning_rate': 0.00015958195265558327, 'batch_size': 32}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:32:23,553] Trial 10 finished with value: 0.7443260073661804 and parameters: {'lstm_units_1': 65, 'lstm_units_2': 50, 'dense_units': 45, 'dropout_rate': 0.11753072230924688, 'learning_rate': 0.0003773756443074937, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:33:10,109] Trial 11 finished with value: 0.7835416674613953 and parameters: {'lstm_units_1': 66, 'lstm_units_2': 50, 'dense_units': 45, 'dropout_rate': 0.10613355198367913, 'learning_rate': 0.0004105295346572048, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:34:08,438] Trial 12 finished with value: 0.777127492427826 and parameters: {'lstm_units_1': 68, 'lstm_units_2': 33, 'dense_units': 48, 'dropout_rate': 0.10874905122541684, 'learning_rate': 0.00033335642891486043, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:35:01,556] Trial 13 finished with value: 0.7004591703414917 and parameters: {'lstm_units_1': 60, 'lstm_units_2': 49, 'dense_units': 37, 'dropout_rate': 0.16622936454321632, 'learning_rate': 0.0003556936842541708, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:35:55,049] Trial 14 finished with value: 0.7059916019439697 and parameters: {'lstm_units_1': 81, 'lstm_units_2': 46, 'dense_units': 51, 'dropout_rate': 0.18530249336146634, 'learning_rate': 0.00047114751017523414, 'batch_size': 64}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:37:17,956] Trial 15 finished with value: 0.6698715686798096 and parameters: {'lstm_units_1': 120, 'lstm_units_2': 35, 'dense_units': 64, 'dropout_rate': 0.1635425169866569, 'learning_rate': 0.00022996612804060203, 'batch_size': 32}. Best is trial 8 with value: 0.7857781291007996.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:38:08,539] Trial 16 finished with value: 0.8274234890937805 and parameters: {'lstm_units_1': 56, 'lstm_units_2': 64, 'dense_units': 17, 'dropout_rate': 0.20519319617317955, 'learning_rate': 0.0030150044330474884, 'batch_size': 64}. Best is trial 16 with value: 0.8274234890937805.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:38:55,623] Trial 17 finished with value: 0.8165557980537415 and parameters: {'lstm_units_1': 53, 'lstm_units_2': 63, 'dense_units': 17, 'dropout_rate': 0.2019650729175729, 'learning_rate': 0.002631296716422727, 'batch_size': 64}. Best is trial 16 with value: 0.8274234890937805.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:39:56,367] Trial 18 finished with value: 0.5821062207221985 and parameters: {'lstm_units_1': 51, 'lstm_units_2': 64, 'dense_units': 16, 'dropout_rate': 0.3545329490181306, 'learning_rate': 0.002590495029568143, 'batch_size': 32}. Best is trial 16 with value: 0.8274234890937805.\n",
      "/home/siddharth/projects/mps/mpsenv/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[I 2025-03-22 15:42:06,607] Trial 19 finished with value: 0.6667597174644471 and parameters: {'lstm_units_1': 33, 'lstm_units_2': 64, 'dense_units': 16, 'dropout_rate': 0.21140622765998923, 'learning_rate': 0.0030946519744895857, 'batch_size': 16}. Best is trial 16 with value: 0.8274234890937805.\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna study to find best hyperparameters\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lstm_units_1': 56, 'lstm_units_2': 64, 'dense_units': 17, 'dropout_rate': 0.20519319617317955, 'learning_rate': 0.0030150044330474884, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset information:\n",
      "Total samples: 4513\n",
      "Feature dimensions: 86\n",
      "Number of classes: 19\n",
      "Class distribution:\n",
      "Class 0: 50 samples\n",
      "Class 1: 156 samples\n",
      "Class 2: 106 samples\n",
      "Class 3: 66 samples\n",
      "Class 4: 6 samples\n",
      "Class 5: 480 samples\n",
      "Class 6: 15 samples\n",
      "Class 7: 124 samples\n",
      "Class 8: 50 samples\n",
      "Class 9: 108 samples\n",
      "Class 10: 667 samples\n",
      "Class 11: 68 samples\n",
      "Class 12: 25 samples\n",
      "Class 13: 571 samples\n",
      "Class 14: 215 samples\n",
      "Class 15: 417 samples\n",
      "Class 16: 6 samples\n",
      "Class 17: 1364 samples\n",
      "Class 18: 19 samples\n"
     ]
    }
   ],
   "source": [
    "# Add these before running the optimization\n",
    "print(\"Dataset information:\")\n",
    "print(\"Total samples:\", len(X_selected))\n",
    "print(\"Feature dimensions:\", X_selected.shape[1])\n",
    "print(\"Number of classes:\", len(np.unique(y)))\n",
    "print(\"Class distribution:\")\n",
    "unique_classes, counts = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 5, n_samples = 5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m smote = SMOTE(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      3\u001b[39m X_train_flat = X_train.reshape(X_train.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_flat, y_train = \u001b[43msmote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m X_train = X_train_flat.reshape(X_train_flat.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, X_selected.shape[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mps/mpsenv/lib/python3.11/site-packages/imblearn/base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mps/mpsenv/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mps/mpsenv/lib/python3.11/site-packages/imblearn/base.py:105\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     99\u001b[39m X, y, binarize_y = \u001b[38;5;28mself\u001b[39m._check_X_y(X, y)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m y_ = (\n\u001b[32m    108\u001b[39m     label_binarize(output[\u001b[32m1\u001b[39m], classes=np.unique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m X_, y_ = arrays_transformer.transform(output[\u001b[32m0\u001b[39m], y_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mps/mpsenv/lib/python3.11/site-packages/imblearn/over_sampling/_smote/base.py:359\u001b[39m, in \u001b[36mSMOTE._fit_resample\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    356\u001b[39m X_class = _safe_indexing(X, target_class_indices)\n\u001b[32m    358\u001b[39m \u001b[38;5;28mself\u001b[39m.nn_k_.fit(X_class)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m nns = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnn_k_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m:]\n\u001b[32m    360\u001b[39m X_new, y_new = \u001b[38;5;28mself\u001b[39m._make_samples(\n\u001b[32m    361\u001b[39m     X_class, y.dtype, class_sample, X_class, nns, n_samples, \u001b[32m1.0\u001b[39m\n\u001b[32m    362\u001b[39m )\n\u001b[32m    363\u001b[39m X_resampled.append(X_new)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/mps/mpsenv/lib/python3.11/site-packages/sklearn/neighbors/_base.py:854\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    852\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    853\u001b[39m         inequality_str = \u001b[33m\"\u001b[39m\u001b[33mn_neighbors <= n_samples_fit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    855\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minequality_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    856\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_neighbors = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, n_samples_fit = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    857\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# include n_samples for common tests\u001b[39;00m\n\u001b[32m    858\u001b[39m     )\n\u001b[32m    860\u001b[39m n_jobs = effective_n_jobs(\u001b[38;5;28mself\u001b[39m.n_jobs)\n\u001b[32m    861\u001b[39m chunked_results = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Expected n_neighbors <= n_samples_fit, but n_neighbors = 6, n_samples_fit = 5, n_samples = 5"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE for class balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_train_flat, y_train = smote.fit_resample(X_train_flat, y_train)\n",
    "X_train = X_train_flat.reshape(X_train_flat.shape[0], 1, X_selected.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test data for LSTM\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_selected.shape[1])\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.Sequential([\n",
    "    Bidirectional(LSTM(best_params['lstm_units_1'], return_sequences=True, input_shape=(1, X_selected.shape[1]))),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    BatchNormalization(),\n",
    "    Bidirectional(LSTM(best_params['lstm_units_2'])),\n",
    "    Dropout(best_params['dropout_rate']),\n",
    "    BatchNormalization(),\n",
    "    Dense(best_params['dense_units'], activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(best_params['dropout_rate']/2),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_lstm_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_flat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = tf.keras.models.load_model('best_lstm_model.h5')  # Load best model saved during training\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "lstm_predictions = np.argmax(lstm_model.predict(X_test), axis=-1)\n",
    "\n",
    "gb_accuracy = gb_model.score(X_test_flat, y_test)\n",
    "gb_predictions = gb_model.predict(X_test_flat)\n",
    "\n",
    "print(f'LSTM Model Accuracy: {lstm_accuracy * 100:.2f}%')\n",
    "print(f'Gradient Boosting Model Accuracy: {gb_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple ensemble (majority voting)\n",
    "ensemble_predictions = np.zeros((X_test.shape[0], len(np.unique(y))))\n",
    "ensemble_predictions += np.eye(len(np.unique(y)))[lstm_predictions]\n",
    "ensemble_predictions += np.eye(len(np.unique(y)))[gb_predictions]\n",
    "final_predictions = np.argmax(ensemble_predictions, axis=1)\n",
    "\n",
    "ensemble_accuracy = np.mean(final_predictions == y_test)\n",
    "print(f'Ensemble Model Accuracy: {ensemble_accuracy * 100:.2f}%')\n",
    "\n",
    "print(\"Classification Report for Ensemble:\")\n",
    "print(classification_report(y_test, final_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(label_encoders, \"label_encoders.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(poly, \"poly_features.pkl\")\n",
    "joblib.dump(selector, \"selector.pkl\")\n",
    "joblib.dump(gb_model, \"gb_model.pkl\")\n",
    "lstm_model.save(\"best_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fertilizer(district, soil_color, nitrogen, phosphorus, potassium, pH, rainfall, temperature, crop):\n",
    "    \"\"\"Predicts the fertilizer based on soil analysis and crop using ensemble model.\"\"\"\n",
    "    # Load models and preprocessing components\n",
    "    lstm_model = tf.keras.models.load_model(\"best_lstm_model.h5\")\n",
    "    gb_model = joblib.load(\"gb_model.pkl\")\n",
    "    label_encoders = joblib.load(\"label_encoders.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    poly = joblib.load(\"poly_features.pkl\")\n",
    "    selector = joblib.load(\"selector.pkl\")\n",
    "    \n",
    "    # Encode categorical inputs\n",
    "    district_encoded = label_encoders['District_Name'].transform([district])[0]\n",
    "    soil_color_encoded = label_encoders['Soil_color'].transform([soil_color])[0]\n",
    "    crop_encoded = label_encoders['Crop'].transform([crop])[0]\n",
    "    \n",
    "    # Create initial input\n",
    "    input_data = pd.DataFrame({\n",
    "        'District_Name': [district_encoded],\n",
    "        'Soil_color': [soil_color_encoded],\n",
    "        'Nitrogen': [nitrogen],\n",
    "        'Phosphorus': [phosphorus],\n",
    "        'Potassium': [potassium],\n",
    "        'pH': [pH],\n",
    "        'Rainfall': [rainfall],\n",
    "        'Temperature': [temperature],\n",
    "        'Crop': [crop_encoded]\n",
    "    })\n",
    "    \n",
    "    # Engineer features (must match training pipeline)\n",
    "    input_data['N_P_ratio'] = input_data['Nitrogen'] / (input_data['Phosphorus'] + 1)\n",
    "    input_data['N_K_ratio'] = input_data['Nitrogen'] / (input_data['Potassium'] + 1)\n",
    "    input_data['P_K_ratio'] = input_data['Phosphorus'] / (input_data['Potassium'] + 1)\n",
    "    input_data['NPK_sum'] = input_data['Nitrogen'] + input_data['Phosphorus'] + input_data['Potassium']\n",
    "    input_data['pH_squared'] = input_data['pH'] ** 2\n",
    "    input_data['N_pH_interaction'] = input_data['Nitrogen'] * input_data['pH']\n",
    "    input_data['P_pH_interaction'] = input_data['Phosphorus'] * input_data['pH']\n",
    "    input_data['K_pH_interaction'] = input_data['Potassium'] * input_data['pH']\n",
    "    input_data['Temp_Rain_ratio'] = input_data['Temperature'] / (input_data['Rainfall'] + 1)\n",
    "    \n",
    "    # Scale, generate polynomial features, and select features\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    input_poly = poly.transform(input_scaled)\n",
    "    input_selected = selector.transform(input_poly)\n",
    "    \n",
    "    # Make predictions from both models\n",
    "    input_lstm = input_selected.reshape(1, 1, input_selected.shape[1])\n",
    "    lstm_pred = np.argmax(lstm_model.predict(input_lstm), axis=-1)[0]\n",
    "    \n",
    "    input_gb = input_selected.reshape(1, -1)\n",
    "    gb_pred = gb_model.predict(input_gb)[0]\n",
    "    \n",
    "    # Ensemble prediction (using the most confident prediction)\n",
    "    lstm_probs = np.max(lstm_model.predict(input_lstm), axis=-1)[0]\n",
    "    gb_probs = np.max(gb_model.predict_proba(input_gb), axis=-1)[0]\n",
    "    \n",
    "    if lstm_probs >= gb_probs:\n",
    "        final_pred = lstm_pred\n",
    "    else:\n",
    "        final_pred = gb_pred\n",
    "    \n",
    "    fertilizer = label_encoders['Fertilizer'].inverse_transform([final_pred])[0]\n",
    "    return fertilizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "predicted_fertilizer = predict_fertilizer(\"Kolhapur\", \"Red\", 50, 30, 20, 6.5, 100, 25, \"Wheat\")\n",
    "print(\"Predicted Fertilizer:\", predicted_fertilizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
